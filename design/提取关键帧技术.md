# 关键帧提取技术综合指南

## 1. 引言

关键帧提取（Keyframe Extraction）是视频分析领域的核心技术之一，其目标是从连续的视频流中识别和提取最具代表性的静态图像帧。这些关键帧能够有效概括视频内容，大幅减少数据存储和传输成本，同时保留视频的核心视觉信息。

关键帧提取在多个应用场景中发挥着重要作用。在视频摘要领域，关键帧可用于生成视频内容的缩略图和预览；在内容检索领域，关键帧作为视频的语义表示，支撑基于内容的视频检索（CBVR）系统；在教育领域，将讲座视频转换为PPT时，关键帧提取能够有效捕捉幻灯片的切换时刻；在视频编辑领域，关键帧用于快速浏览和导航长视频内容；在监控领域，关键帧可大幅降低海量监控视频的存储和分析成本。

关键帧提取面临的核心挑战包括：如何定义"代表性"的度量标准、如何平衡提取的完整性与冗余去除、如何处理不同类型视频内容的差异性、以及如何在实时性和准确性之间取得平衡。根据应用场景的不同，关键帧提取算法需要针对特定目标进行优化，如最大化内容覆盖率、最小化信息冗余、或保持时间顺序的连贯性。

## 2. 现有方法：基于相似度的方案

本项目采用的方案源自开源项目 [`extract-video-ppt`](https://github.com/wudududu/extract-video-ppt)，该方法专注于将视频演示文稿高效转换为PDF文档。

### 2.1 核心算法原理

该方法采用基于相似度过滤的策略，通过帧间差异性判断来确定哪些帧应被保留。其工作流程分为四个阶段：

**帧采样阶段**：按照固定时间间隔从视频中提取帧序列。采样间隔可根据视频长度和预期输出规模进行调整，通常设置为1-5秒可获得较好的效果。

**相似度计算阶段**：对每个新提取的帧，计算其与上一帧（已保存的最后一个帧）之间的相似度。算法支持多种相似度度量方式，默认采用结构相似性指数（SSIM）或像素差异度作为度量标准。

**阈值判断阶段**：将计算得到的相似度与预设阈值进行比较。当相似度低于阈值（默认0.6）时，表明当前帧与已保存帧存在显著差异，应被保留；当相似度高于阈值时，当前帧被视为冗余信息而被忽略。

**PDF合成阶段**：将所有通过筛选的关键帧按时间顺序合成PDF文档，支持单页单帧或多帧拼贴的输出格式。

### 2.2 算法实现伪代码

```python
def extract_keyframes(video_path, similarity_threshold=0.6, 
                      start_time="00:00:00", end_time=None, fps=1):
    """
    基于相似度的关键帧提取算法
    
    参数:
        video_path: 视频文件路径
        similarity_threshold: 相似度阈值 (0-1)，默认0.6
        start_time: 起始时间
        end_time: 结束时间，None表示直到视频结束
        fps: 每秒提取帧数
    
    返回:
        keyframes: 关键帧列表
    """
    keyframes = []
    last_saved_frame = None
    
    cap = cv2.VideoCapture(video_path)
    fps_video = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps_video / fps)
    
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # 时间范围过滤
        current_time = frame_count / fps_video
        if current_time < parse_time(start_time):
            frame_count += 1
            continue
        if end_time and current_time > parse_time(end_time):
            break
        
        # 跳帧采样
        if frame_count % frame_interval != 0:
            frame_count += 1
            continue
        
        # 计算与上一关键帧的相似度
        if last_saved_frame is not None:
            similarity = calculate_similarity(frame, last_saved_frame)
            
            if similarity < similarity_threshold:
                keyframes.append(frame)
                last_saved_frame = frame.copy()
        else:
            # 第一帧总是保存
            keyframes.append(frame)
            last_saved_frame = frame.copy()
        
        frame_count += 1
    
    cap.release()
    return keyframes

def calculate_similarity(frame1, frame2):
    """
    计算两帧之间的相似度
    使用SSIM (结构相似性指数)
    """
    # 转换为灰度图
    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
    
    # 计算SSIM
    similarity = cv2.matchTemplate(gray1, gray2, cv2.TM_CCOEFF_NORMED)[0][0]
    return similarity
```

### 2.3 命令行参数说明

```bash
# 基本用法
evp --similarity 0.6 --pdfname output.pdf input_video.mp4

# 指定时间范围
evp --similarity 0.6 \
    --pdfname presentation.pdf \
    --start_frame 0:00:09 \
    --end_frame 00:30:00 \
    ./input_video.mp4

# 参数详解
--similarity 0.6     # 相似度阈值，范围0-1，值越小保留的帧越多
--pdfname hello.pdf  # 输出PDF文件名
--start_frame 0:00:00  # 起始时间点，格式HH:MM:SS
--end_frame 00:00:30   # 结束时间点
```

### 2.4 适用场景与局限性

**适用场景**：
- 教育讲座视频：幻灯片切换清晰的演示内容
- 会议演示：PPT风格的正式汇报
- 产品发布：画面稳定、过渡明显的视频

**局限性分析**：
- **语义理解缺失**：算法仅基于视觉差异进行判断，无法理解帧的实际内容含义
- **过渡帧敏感**：对于渐变式切换（淡入淡出）可能产生过多或过少的帧
- **阈值依赖性强**：不同类型的视频需要调整不同的阈值参数
- **相机运动干扰**：视频中的镜头移动会导致误检为关键帧

**优化建议**：
- 对演讲类视频，可将阈值调低至0.5-0.55以捕获更多幻灯片内容
- 对动态场景视频，建议结合运动补偿或光流分析进行预处理
- 可增加滑动窗口机制，对局部相似度进行平滑处理

## 3. 传统计算机视觉方法

### 3.1 直方图比较法

直方图比较是视频分析中最基础且广泛应用的关键帧提取方法。该方法通过分析图像的颜色分布特征来判断帧间的相似程度，具有计算效率高、对位置变化不敏感等优点。

**颜色直方图提取**：将图像从RGB色彩空间转换到HSV或Lab空间，统计各通道的像素分布。HSV空间因其对光照变化的鲁棒性而常被选用，其中H通道（色相）对颜色描述尤为重要。

**相似度度量方法**：

卡方距离（Chi-Square Distance）是最常用的直方图比较度量，其计算公式为：

$$d(H_1, H_2) = \sum_i \frac{(H_1(i) - H_2(i))^2}{H_1(i) + H_2(i)}$$

巴氏距离（Bhattacharyya Distance）适用于归一化直方图：

$$d(H_1, H_2) = \sqrt{1 - \sum_i \frac{\sqrt{H_1(i) \cdot H_2(i)}}{\sqrt{\sum_j H_1(j) \cdot \sum_k H_2(k)}}}$$

```python
import cv2
import numpy as np

def extract_histogram_features(frame, bins=(8, 8, 8)):
    """
    提取HSV颜色直方图特征
    
    参数:
        frame: BGR格式图像
        bins: 各通道的bin数 (H, S, V)
    
    返回:
        hist: 归一化直方图
    """
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])
    cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)
    return hist

def chi_square_distance(hist1, hist2):
    """计算卡方距离"""
    distance = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CHISQR)
    return distance

def bhattacharyya_distance(hist1, hist2):
    """计算巴氏距离"""
    distance = cv2.compareHist(hist1, hist2, cv2.HISTCMP_BHATTACHARYYA)
    return distance

def histogram_based_keyframe_extraction(video_path, threshold=0.7):
    """
    基于直方图的关键帧提取
    
    参数:
        video_path: 视频路径
        threshold: 直方图相似度阈值
    """
    cap = cv2.VideoCapture(video_path)
    keyframes = []
    last_hist = None
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        current_hist = extract_histogram_features(frame)
        
        if last_hist is not None:
            similarity = 1 - bhattacharyya_distance(last_hist, current_hist)
            
            if similarity < threshold:
                keyframes.append(frame.copy())
                last_hist = current_hist
        else:
            keyframes.append(frame.copy())
            last_hist = current_hist
    
    cap.release()
    return keyframes
```

### 3.2 边缘检测算法

边缘变化率（Edge Change Ratio, ECR）是基于边缘信息的镜头边界检测方法，通过比较相邻帧的边缘分布变化来识别场景切换。

**Canny边缘检测**：Canny算子通过高斯滤波去噪、计算梯度幅值和方向、非极大值抑制、以及双阈值检测五个步骤提取图像边缘。

**边缘变化率计算**：分别计算当前帧相对于前一帧的新增边缘比例和消失边缘比例，取两者最大值作为边缘变化率：

$$ECR = \max\left(\frac{E_{new}}{E_{total}}, \frac{E_{lost}}{E_{prev}}\right)$$

当ECR值超过预设阈值（通常为0.2-0.4）时，判定为场景切换点。

```python
def canny_edge_detection(frame, low_threshold=100, high_threshold=200):
    """
    Canny边缘检测
    
    参数:
        frame: 输入图像 (BGR格式)
        low_threshold: 低阈值
        high_threshold: 高阈值
    
    返回:
        edges: 二值边缘图像
    """
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, low_threshold, high_threshold)
    return edges

def calculate_edge_change_ratio(edges1, edges2):
    """
    计算边缘变化率 (ECR)
    
    参数:
        edges1: 前一帧的边缘图
        edges2: 当前帧的边缘图
    
    返回:
        ecr: 边缘变化率
    """
    # 确保尺寸一致
    if edges1.shape != edges2.shape:
        edges2 = cv2.resize(edges2, (edges1.shape[1], edges1.shape[0]))
    
    # 计算新增边缘和消失边缘
    new_edges = np.logical_and(edges2, np.logical_not(edges1))
    lost_edges = np.logical_and(edges1, np.logical_not(edges2))
    
    # 计算变化率
    e_new = np.sum(new_edges) / (edges1.shape[0] * edges1.shape[1] + 1e-10)
    e_lost = np.sum(lost_edges) / (edges1.shape[0] * edges1.shape[1] + 1e-10)
    
    ecr = max(e_new, e_lost)
    return ecr

def edge_based_keyframe_extraction(video_path, ecr_threshold=0.3):
    """
    基于边缘变化率的关键帧提取
    """
    cap = cv2.VideoCapture(video_path)
    keyframes = []
    prev_edges = None
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        current_edges = canny_edge_detection(frame)
        
        if prev_edges is not None:
            ecr = calculate_edge_change_ratio(prev_edges, current_edges)
            
            if ecr > ecr_threshold:
                keyframes.append(frame.copy())
        
        prev_edges = current_edges
    
    cap.release()
    return keyframes
```

### 3.3 运动估计技术

光流法（Optical Flow）通过分析像素在连续帧间的运动矢量来检测视频中的运动模式和场景变化。稠密光流（如Farneback算法）可生成完整的运动矢量场，稀疏光流（如Lucas-Kanade算法）则专注于特征点的跟踪。

**Farneback稠密光流**：该方法基于多项式展开模型，能够高效计算每像素的运动矢量。通过分析运动矢量的统计特征（如平均幅值、方向一致性），可识别镜头切换和显著场景变化。

```python
def compute_farneback_flow(frame1, frame2,pyr_scale=0.5, levels=3, 
                           winsize=15, iterations=3, poly_n=5, poly_sigma=1.2):
    """
    计算Farneback稠密光流
    
    参数:
        frame1, frame2: 连续两帧
        pyr_scale: 金字塔尺度因子
        levels: 金字塔层数
        winsize: 平均窗口大小
        iterations: 迭代次数
    
    返回:
        flow: 光流场 (H, W, 2)
    """
    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
    
    flow = cv2.calcOpticalFlowFarneback(
        gray1, gray2, None, pyr_scale, levels, winsize, 
        iterations, poly_n, poly_sigma, 0
    )
    return flow

def motion_analysis_keyframe_extraction(video_path, motion_threshold=2.0):
    """
    基于光流运动分析的关键帧提取
    
    通过检测全局运动模式的突然变化来识别关键帧
    """
    cap = cv2.VideoCapture(video_path)
    keyframes = []
    prev_flow_magnitude = None
    
    ret, prev_frame = cap.read()
    if not ret:
        return keyframes
    
    while True:
        ret, curr_frame = cap.read()
        if not ret:
            break
        
        flow = compute_farneback_flow(prev_frame, curr_frame)
        
        # 计算光流幅值的统计特征
        magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        avg_magnitude = np.mean(magnitude)
        std_magnitude = np.std(magnitude)
        
        # 基于运动统计判断是否为关键帧
        if prev_flow_magnitude is not None:
            motion_change = abs(avg_magnitude - prev_flow_magnitude)
            
            # 运动突变或运动模式剧烈变化
            if motion_change > motion_threshold or std_magnitude > 5.0:
                keyframes.append(curr_frame.copy())
        
        prev_flow_magnitude = avg_magnitude
        prev_frame = curr_frame
    
    cap.release()
    return keyframes
```

### 3.4 场景切换检测

场景切换检测（Shot Boundary Detection）的核心任务是识别视频中不同镜头或场景的边界。主要技术路线包括帧差法、像素变化检测、以及前述的直方图和边缘方法。

**帧差法**：计算连续帧之间的像素级差异，通过阈值判断场景切换。

```python
def frame_difference_keyframe_extraction(video_path, diff_threshold=30, 
                                          pixel_ratio_threshold=0.3):
    """
    基于帧差法的场景切换检测
    
    参数:
        diff_threshold: 像素差异阈值
        pixel_ratio_threshold: 显著变化像素比例阈值
    """
    cap = cv2.VideoCapture(video_path)
    keyframes = []
    prev_frame = None
    
    while True:
        ret, curr_frame = cap.read()
        if not ret:
            break
        
        if prev_frame is not None:
            # 计算灰度差分图
            gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            gray_curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
            
            diff = cv2.absdiff(gray_prev, gray_curr)
            
            # 阈值化处理
            _, thresh = cv2.threshold(diff, diff_threshold, 255, cv2.THRESH_BINARY)
            
            # 计算变化像素比例
            changed_ratio = np.sum(thresh > 0) / thresh.size
            
            if changed_ratio > pixel_ratio_threshold:
                keyframes.append(curr_frame.copy())
        
        prev_frame = curr_frame
    
    cap.release()
    return keyframes
```

## 4. 深度学习方法

### 4.1 基于CNN的特征提取

卷积神经网络（CNN）通过层级化的特征学习，能够捕获从低级纹理到高级语义的多层次视觉信息。预训练的CNN模型（如ResNet、VGG）提取的特征向量可有效表征视频帧的内容特性。

**ResNet特征提取**：ResNet的残差连接使其能够训练更深的网络，从而学习到更丰富的视觉特征。最后一层全连接前的特征向量（通常为2048维）是通用的视觉表示。

```python
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights

class CNNFeatureExtractor:
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
        self.model.eval()
        self.model.to(self.device)
        
        # 移除最后的分类层，获取特征向量
        self.feature_extractor = torch.nn.Sequential(
            *list(self.model.children())[:-1]
        )
        
        self.preprocess = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
    
    def extract_features(self, frame):
        """提取单帧的CNN特征"""
        tensor = self.preprocess(frame).unsqueeze(0).to(self.device)
        with torch.no_grad():
            features = self.feature_extractor(tensor)
        return features.squeeze().cpu().numpy()

def cnn_based_keyframe_extraction(video_path, similarity_threshold=0.85):
    """
    基于CNN特征的关键帧提取
    
    使用余弦相似度比较帧间特征差异
    """
    extractor = CNNFeatureExtractor()
    cap = cv2.VideoCapture(video_path)
    keyframes = []
    last_features = None
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        current_features = extractor.extract_features(frame)
        
        if last_features is not None:
            # 计算余弦相似度
            similarity = np.dot(current_features, last_features) / (
                np.linalg.norm(current_features) * np.linalg.norm(last_features)
            )
            
            if similarity < similarity_threshold:
                keyframes.append(frame.copy())
                last_features = current_features
        else:
            keyframes.append(frame.copy())
            last_features = current_features
    
    cap.release()
    return keyframes
```

### 4.2 时序分段网络（TSN）

时序分段网络（Temporal Segment Networks, TSN）通过在视频的多个时间片段中稀疏采样，并对各片段的特征进行聚合，实现对视频的时序建模。该方法特别适用于需要理解视频动态内容的场景。

**核心思想**：将视频划分为K个片段，从每个片段中随机采样一帧，通过2D CNN提取各帧的空间特征，然后通过时序聚合（如平均池化）得到视频级表示。

```python
import torch
import torch.nn.functional as F

class TemporalSegmentNetwork:
    def __init__(self, backbone='resnet50', num_segments=3):
        self.num_segments = num_segments
        self.feature_extractor = CNNFeatureExtractor()
    
    def extract_video_features(self, frames):
        """
        提取视频的时序特征
        
        参数:
            frames: 视频帧列表
        
        返回:
            video_feature: 聚合的视频特征
        """
        if len(frames) == 0:
            return None
        
        segment_features = []
        segment_size = len(frames) // self.num_segments
        
        for i in range(self.num_segments):
            # 从每个片段中采样中间帧
            idx = min(i * segment_size + segment_size // 2, len(frames) - 1)
            frame = frames[idx]
            features = self.feature_extractor.extract_features(frame)
            segment_features.append(features)
        
        # 聚合特征（时序平均池化）
        video_feature = np.mean(segment_features, axis=0)
        
        return video_feature
    
    def select_keyframes_tsn(self, all_frames, threshold=0.9):
        """
        基于TSN的关键帧选择
        
        使用滑动窗口策略，检测视频表示的显著变化
        """
        window_size = 10
        keyframe_indices = []
        video_features = []
        
        # 提取每个帧的特征
        for i, frame in enumerate(all_frames):
            features = self.feature_extractor.extract_features(frame)
            video_features.append(features)
        
        # 滑动窗口检测关键帧
        for i in range(len(video_features)):
            if i < window_size:
                keyframe_indices.append(0)
                continue
            
            # 计算当前窗口与前一窗口的差异
            window_current = video_features[max(0, i-window_size):i+1]
            window_prev = video_features[max(0, i-2*window_size):max(0, i-window_size)]
            
            feat_current = np.mean(window_current, axis=0)
            feat_prev = np.mean(window_prev, axis=0)
            
            similarity = np.dot(feat_current, feat_prev) / (
                np.linalg.norm(feat_current) * np.linalg.norm(feat_prev)
            )
            
            if similarity < threshold:
                keyframe_indices.append(i)
        
        return [all_frames[i] for i in keyframe_indices]
```

### 4.3 视频Transformer

视频Transformer利用自注意力机制（Self-Attention）建模帧间的全局依赖关系，能够捕获长距离的时序关联。代表工作包括TimeSformer、Video Swin Transformer等。

**时空注意力机制**：区别于传统CNN的局部感受野，Transformer能够直接建模视频中任意两帧之间的关系，实现全局信息聚合。

```python
import torch
from transformers import TimesformerForVideoClassification, TimesformerModel

class VideoTransformerExtractor:
    def __init__(self, model_name='facebook/timesformer-base-finetuned-k400'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = TimesformerModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
    
    def extract_video_features(self, video_frames, num_frames=8):
        """
        提取视频的Transformer特征
        
        参数:
            video_frames: 视频帧列表
            num_frames: 采样的帧数
        
        返回:
            video_features: 视频级特征表示
        """
        # 采样帧
        indices = np.linspace(0, len(video_frames) - 1, num_frames, dtype=int)
        sampled_frames = [video_frames[i] for i in indices]
        
        # 预处理
        inputs = self.preprocess_frames(sampled_frames)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # 使用[CLS]token作为视频表示
            video_features = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        
        return video_features.squeeze()
    
    def preprocess_frames(self, frames):
        """预处理帧序列"""
        # 实现视频预处理逻辑
        pass

def transformer_keyframe_selection(video_path, threshold=0.85):
    """
    基于Transformer的关键帧选择
    """
    extractor = VideoTransformerExtractor()
    cap = cv2.VideoCapture(video_path)
    
    all_frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        all_frames.append(frame.copy())
    cap.release()
    
    # 使用Transformer选择关键帧
    selector = TemporalSegmentNetwork()
    keyframes = selector.select_keyframes_tsn(all_frames, threshold)
    
    return keyframes
```

### 4.4 CLIP预训练模型

CLIP（Contrastive Language-Image Pre-training）通过对比学习对齐视觉和文本表示，具有强大的零样本视觉理解能力。将CLIP应用于关键帧提取，可实现语义感知的帧选择。

```python
import clip
import torch

class CLIPKeyframeExtractor:
    def __init__(self, model_name='ViT-B/32'):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        self.model.eval()
    
    def extract_frame_features(self, frame):
        """提取单帧的CLIP特征"""
        image = self.preprocess(frame).unsqueeze(0).to(self.device)
        with torch.no_grad():
            image_features = self.model.encode_image(image)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        return image_features.cpu().numpy().squeeze()
    
    def semantic_keyframe_extraction(self, video_path, prompt="keyframe that represents a scene change",
                                      threshold=0.75):
        """
        基于CLIP语义理解的关键帧提取
        
        通过文本提示引导关键帧选择
        """
        cap = cv2.VideoCapture(video_path)
        keyframes = []
        last_features = None
        
        # 将文本提示编码为特征向量
        text_tokens = clip.tokenize([prompt]).to(self.device)
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_features = self.extract_frame_features(frame)
            
            # 计算与文本提示的相关性
            text_similarity = np.dot(current_features, text_features.squeeze())
            
            if last_features is not None:
                frame_similarity = np.dot(current_features, last_features)
                
                # 当帧间差异大且与语义提示相关时选择为关键帧
                if frame_similarity < threshold and text_similarity > 0.2:
                    keyframes.append(frame.copy())
                    last_features = current_features
            else:
                keyframes.append(frame.copy())
                last_features = current_features
        
        cap.release()
        return keyframes
```

## 5. 学术研究方法

### 5.1 聚类方法

聚类方法将相似的帧分组，选取每组中最具代表性的帧作为关键帧。K-means和DBSCAN是两种常用的聚类策略。

**K-means聚类**：将视频帧特征空间划分为K个簇，选择每个簇的质心帧或距离质心最近的帧作为关键帧。K值的选择是关键挑战，通常需要通过肘部法则或轮廓系数确定。

```python
from sklearn.cluster import KMeans, MiniBatchKMeans

def kmeans_keyframe_extraction(video_path, num_keyframes=10):
    """
    基于K-means聚类的关键帧提取
    
    参数:
        video_path: 视频路径
        num_keyframes: 目标关键帧数量（即K值）
    
    返回:
        keyframes: 关键帧列表
        cluster_labels: 每帧的聚类标签
    """
    extractor = CNNFeatureExtractor()
    cap = cv2.VideoCapture(video_path)
    
    # 提取所有帧的特征
    all_features = []
    frame_indices = []
    
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # 每5帧采样一次以提高效率
        if frame_count % 5 == 0:
            features = extractor.extract_features(frame)
            all_features.append(features)
            frame_indices.append(frame_count)
        
        frame_count += 1
    cap.release()
    
    all_features = np.array(all_features)
    
    # K-means聚类
    kmeans = MiniBatchKMeans(n_clusters=num_keyframes, random_state=42, batch_size=100)
    cluster_labels = kmeans.fit_predict(all_features)
    cluster_centers = kmeans.cluster_centers_
    
    # 为每个簇选择最近的帧
    keyframes = []
    for i in range(num_keyframes):
        cluster_mask = cluster_labels == i
        cluster_features = all_features[cluster_mask]
        cluster_indices = np.array(frame_indices)[cluster_mask]
        
        if len(cluster_features) > 0:
            # 找到距离质心最近的帧
            center = cluster_centers[i].reshape(1, -1)
            distances = np.linalg.norm(cluster_features - center, axis=1)
            best_idx = np.argmin(distances)
            best_frame_idx = cluster_indices[best_idx]
            
            # 重新读取该帧
            cap_temp = cv2.VideoCapture(video_path)
            for _ in range(best_frame_idx):
                cap_temp.read()
            ret, keyframe = cap_temp.read()
            if ret:
                keyframes.append(keyframe)
            cap_temp.release()
    
    return keyframes, cluster_labels
```

**DBSCAN密度聚类**：DBSCAN基于密度可达性进行聚类，无需预设簇数量，能够自动发现任意形状的簇，并识别异常帧。

```python
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

def dbscan_keyframe_extraction(video_path, eps=0.5, min_samples=5):
    """
    基于DBSCAN聚类的关键帧提取
    
    参数:
        eps: 邻域半径
        min_samples: 核心点的最小邻居数
    """
    extractor = CNNFeatureExtractor()
    cap = cv2.VideoCapture(video_path)
    
    # 提取特征
    all_features = []
    frame_indices = []
    
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        if frame_count % 5 == 0:
            features = extractor.extract_features(frame)
            all_features.append(features)
            frame_indices.append(frame_count)
        
        frame_count += 1
    cap.release()
    
    all_features = np.array(all_features)
    
    # 标准化特征
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(all_features)
    
    # DBSCAN聚类
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    cluster_labels = dbscan.fit_predict(features_scaled)
    
    # 选择每个簇的代表帧
    keyframes = []
    unique_labels = set(cluster_labels)
    
    for label in unique_labels:
        if label == -1:  # 噪声点
            continue
        
        cluster_mask = cluster_labels == label
        cluster_indices = np.array(frame_indices)[cluster_mask]
        
        # 选择簇中最早的帧作为代表
        best_frame_idx = min(cluster_indices)
        
        cap_temp = cv2.VideoCapture(video_path)
        for _ in range(best_frame_idx):
            cap_temp.read()
        ret, keyframe = cap_temp.read()
        if ret:
            keyframes.append(keyframe)
        cap_temp.release()
    
    return keyframes, cluster_labels
```

### 5.2 最大边际相关性（MMR）

最大边际相关性算法通过迭代选择与已选关键帧语义距离最大、同时与查询/视频整体相关性最高的帧，有效平衡信息量和多样性。

```python
import numpy as np

def max_marginal_relevance_keyframe_selection(frame_features, num_keyframes, 
                                               lambda_param=0.7):
    """
    基于MMR的关键帧选择算法
    
    参数:
        frame_features: 所有帧的特征矩阵 (N, D)
        num_keyframes: 需要选择的关键帧数量
        lambda_param: 平衡参数，越高越强调相关性
    
    返回:
        selected_indices: 选中的关键帧索引
    """
    num_frames = frame_features.shape[0]
    num_keyframes = min(num_keyframes, num_frames)
    
    # 归一化特征向量
    norms = np.linalg.norm(frame_features, axis=1, keepdims=True)
    features_norm = frame_features / (norms + 1e-10)
    
    # 计算相似度矩阵
    similarity_matrix = features_norm @ features_norm.T
    
    # 选择第一个关键帧（与整体平均相似度最低的帧）
    avg_similarities = np.mean(similarity_matrix, axis=1)
    first_idx = np.argmin(avg_similarities)
    
    selected = [first_idx]
    remaining = list(range(num_frames))
    remaining.remove(first_idx)
    
    # 迭代选择
    while len(selected) < num_keyframes and remaining:
        best_score = -np.inf
        best_idx = None
        
        for idx in remaining:
            # 相关性得分（与已选帧的平均相似度）
            relevance = np.mean([similarity_matrix[idx, s] for s in selected])
            
            # 多样性惩罚（与已选帧的最大相似度）
            diversity = np.max([similarity_matrix[idx, s] for s in selected])
            
            # MMR公式
            score = lambda_param * relevance - (1 - lambda_param) * diversity
            
            if score > best_score:
                best_score = score
                best_idx = idx
        
        if best_idx is None:
            break
            
        selected.append(best_idx)
        remaining.remove(best_idx)
    
    return selected

def mmr_keyframe_extraction(video_path, num_keyframes=10):
    """使用MMR方法提取关键帧"""
    extractor = CNNFeatureExtractor()
    cap = cv2.VideoCapture(video_path)
    
    # 提取特征
    all_features = []
    frame_indices = []
    
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        if frame_count % 3 == 0:
            features = extractor.extract_features(frame)
            all_features.append(features)
            frame_indices.append(frame_count)
        
        frame_count += 1
    cap.release()
    
    all_features = np.array(all_features)
    
    # MMR选择
    selected_indices = max_marginal_relevance_keyframe_selection(
        all_features, num_keyframes
    )
    
    # 获取对应的帧
    keyframes = []
    for idx in selected_indices:
        frame_idx = frame_indices[idx]
        
        cap_temp = cv2.VideoCapture(video_path)
        for _ in range(frame_idx):
            cap_temp.read()
        ret, keyframe = cap_temp.read()
        if ret:
            keyframes.append(keyframe)
        cap_temp.release()
    
    return keyframes, selected_indices
```

### 5.3 基于注意力的方法

注意力机制通过学习帧的重要性权重，实现自适应关键帧选择。自注意力机制能够捕获帧间的长程依赖关系。

```python
import torch
import torch.nn as nn

class AttentionBasedKeyframeSelector(nn.Module):
    def __init__(self, feature_dim=2048, num_heads=8, num_layers=2):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            batch_first=True
        )
        self.fc = nn.Linear(feature_dim, 1)
        self.softmax = nn.Softmax(dim=0)
    
    def forward(self, features):
        """
        计算每个帧的重要性分数
        
        参数:
            features: 帧特征序列 (N, D)
        
        返回:
            attention_scores: 每帧的注意力分数 (N,)
        """
        # 添加位置编码
        position编码 = self._create_position_encoding(features.size(0), features.size(1))
        features_with_pos = features + position编码
        
        # 自注意力
        attn_output, _ = self.attention(
            features_with_pos, features_with_pos, features_with_pos
        )
        
        # 聚合为单一分数
        scores = self.fc(attn_output).squeeze(-1)
        attention_scores = self.softmax(scores)
        
        return attention_scores
    
    def _create_position_encoding(self, seq_len, embed_dim):
        """创建正弦位置编码"""
        position = torch.arange(seq_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2) * 
                           (-np.log(10000.0) / embed_dim))
        pe = torch.zeros(seq_len, embed_dim)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe

def attention_keyframe_extraction(video_path, top_k=10, threshold=0.05):
    """
    基于注意力机制的关键帧提取
    """
    extractor = CNNFeatureExtractor()
    cap = cv2.VideoCapture(video_path)
    
    # 提取特征序列
    all_features = []
    frame_indices = []
    
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        if frame_count % 3 == 0:
            features = extractor.extract_features(frame)
            all_features.append(features)
            frame_indices.append(frame_count)
        
        frame_count += 1
    cap.release()
    
    all_features = torch.tensor(np.array(all_features), dtype=torch.float32)
    
    # 注意力评分
    selector = AttentionBasedKeyframeSelector()
    attention_scores = selector(all_features)
    
    # 基于分数选择关键帧
    # 策略1：选择top-k最高分帧
    top_k_indices = torch.topk(attention_scores, min(top_k, len(attention_scores))).indices.numpy()
    
    # 策略2：选择分数超过阈值的帧
    threshold_indices = np.where(attention_scores.numpy() > threshold)[0]
    
    selected_indices = sorted(set(top_k_indices.tolist()) | set(threshold_indices.tolist()))
    
    # 获取对应的帧
    keyframes = []
    for idx in selected_indices:
        frame_idx = frame_indices[idx]
        
        cap_temp = cv2.VideoCapture(video_path)
        for _ in range(frame_idx):
            cap_temp.read()
        ret, keyframe = cap_temp.read()
        if ret:
            keyframes.append(keyframe)
        cap_temp.release()
    
    return keyframes, attention_scores.numpy()
```

### 5.4 大视觉语言模型方法

LMSKE（Large Model-based Keyframe Extraction）等方法利用大规模视觉语言模型（如GPT-4V、Gemini）实现语义理解驱动的关键帧提取，能够根据自然语言描述的内容需求选择最相关的帧。

```python
class LMSKEKeyframeExtractor:
    """
    基于大视觉语言模型的关键帧提取
    
    使用VLM分析每帧的语义内容，
    选择能完整覆盖视频叙事结构的帧
    """
    
    def __init__(self, vlm_model='gpt-4v'):
        self.vlm_model = vlm_model
        self.client = None  # 初始化对应的API客户端
    
    def analyze_frame_semantics(self, frame, context=""):
        """使用VLM分析单帧的语义内容"""
        # 调用VLM API分析帧内容
        pass
    
    def semantic_coverage_selection(self, video_path, num_keyframes=10,
                                     video_description="请描述视频的主要内容"):
        """
        基于语义覆盖率的关键帧选择
        
        1. 使用VLM分析整个视频的叙事结构
        2. 识别视频的关键叙事节点
        3. 选择覆盖这些节点的帧
        """
        # 1. 粗采样获取候选帧
        cap = cv2.VideoCapture(video_path)
        candidate_frames = []
        frame_indices = []
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_indices = np.linspace(0, total_frames - 1, num_keyframes * 3, dtype=int)
        
        for idx in sample_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                candidate_frames.append(frame)
                frame_indices.append(idx)
        
        cap.release()
        
        # 2. 使用VLM评估每帧的叙事重要性
        frame_importance = []
        for frame, idx in zip(candidate_frames, frame_indices):
            # 调用VLM进行语义分析
            importance = self._query_frame_importance(frame, idx, video_description)
            frame_importance.append(importance)
        
        # 3. 选择重要性最高的帧，确保时间分布均匀
        selected = self._diverse_sampling(frame_importance, frame_indices, num_keyframes)
        
        return selected
    
    def _query_frame_importance(self, frame, frame_idx, video_description):
        """查询VLM评估帧的重要性"""
        # 实现VLM API调用
        pass
    
    def _diverse_sampling(self, importance_scores, frame_indices, num_keyframes):
        """确保时间分布均匀的多样性采样"""
        # 重要性加权的时间均匀采样
        pass
```

## 6. 行业工具与库

### 6.1 OpenCV

OpenCV是最广泛使用的计算机视觉库，提供了丰富的视频处理和图像分析功能。

```python
import cv2

class OpenCVKeyframeExtractor:
    """使用OpenCV进行关键帧提取"""
    
    def extract_by_frame_difference(self, video_path, threshold=30, 
                                     pixel_ratio=0.3):
        """基于帧差法的关键帧提取"""
        cap = cv2.VideoCapture(video_path)
        keyframes = []
        prev_frame = None
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if prev_frame is not None:
                # 计算帧差
                diff = cv2.absdiff(
                    cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY),
                    cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                )
                
                # 计算变化像素比例
                changed = np.sum(diff > threshold) / diff.size
                
                if changed > pixel_ratio:
                    keyframes.append(frame.copy())
            
            prev_frame = frame.copy()
        
        cap.release()
        return keyframes
    
    def extract_by_histogram(self, video_path, threshold=0.7):
        """基于直方图的关键帧提取"""
        cap = cv2.VideoCapture(video_path)
        keyframes = []
        prev_hist = None
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # 计算HSV直方图
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            hist = cv2.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])
            cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)
            
            if prev_hist is not None:
                # 巴氏距离比较
                similarity = 1 - cv2.compareHist(prev_hist, hist, 
                                                  cv2.HISTCMP_BHATTACHARYYA)
                
                if similarity < threshold:
                    keyframes.append(frame.copy())
                    prev_hist = hist
            else:
                keyframes.append(frame.copy())
                prev_hist = hist
        
        cap.release()
        return keyframes
```

### 6.2 FFmpeg场景检测

FFmpeg是强大的多媒体处理工具，内置了场景检测功能。

```bash
# 使用FFmpeg进行场景检测和关键帧提取

# 方法1：使用select滤镜进行场景检测
ffmpeg -i input.mp4 -vf "select='gt(scene,0.4)',showinfo" -fps_mode vfr frames_%04d.png

# 方法2：提取所有I帧（关键帧）
ffmpeg -i input.mp4 -vf "select=eq(pict_type\,I)" -fps_mode vfr keyframes_%04d.png

# 方法3：使用场景检测滤镜
ffmpeg -i input.mp4 -filter_complex "[0:v] showinfo, select='gt(scene,0.6)'[out]" \
    -map "[out]" keyframes_%04d.jpg

# 方法4：结合场景检测和缩略图生成
ffmpeg -i input.mp4 \
    -vf "select='gt(scene,0.5)',scale=320:-1,tile=5x5" \
    -frames:v 1 \
    thumbnail.jpg
```

### 6.3 PySceneDetect

PySceneDetect是专门用于场景检测的Python库，提供了多种检测器。

```python
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector, ThresholdDetector
from scenedetect.video_splitter import split_video_screenshots

# 使用ContentDetector（基于内容变化的检测）
def extract_keyframes_with_scenecut(video_path, threshold=30.0):
    """
    使用PySceneDetect的内容检测器提取关键帧
    
    参数:
        video_path: 视频路径
        threshold: 内容变化阈值
    """
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    
    # 添加内容检测器
    scene_manager.add_detector(
        ContentDetector(threshold=threshold)
    )
    
    video_manager.set_downscale_factor()
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    
    # 获取场景列表
    scene_list = scene_manager.get_scene_list()
    
    # 为每个场景提取代表帧
    keyframes = []
    for scene_start, scene_end in scene_list:
        # 提取场景中间帧
        middle_frame = (scene_start + scene_end) // 2
        keyframes.append(middle_frame)
    
    return keyframes, scene_list

# 使用ThresholdDetector（基于像素变化的检测）
def extract_keyframes_with_threshold(video_path, threshold=15, 
                                      min_scene_len=15):
    """
    使用PySceneDetect的阈值检测器
    """
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    
    scene_manager.add_detector(
        ThresholdDetector(threshold=threshold, min_scene_len=min_scene_len)
    )
    
    video_manager.set_downscale_factor()
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    
    scene_list = scene_manager.get_scene_list()
    return scene_list
```

### 6.4 KATNA

Keyframe and TranscriptKATNA（ Annotation）是KDnuggets推出的自动化关键帧提取工具，注重提取质量。

```python
from katna.video import Video
from katna.frame_selector import FrameSelector

def katna_keyframe_extraction(video_path, num_keyframes=10):
    """
    使用KATNA进行质量感知的关键帧提取
    
    KATNA综合考虑图像质量、构图、清晰度等因素
    """
    video = Video()
    selector = FrameSelector()
    
    # 加载视频
    video.load_video(video_path)
    
    # 提取关键帧
    keyframes = selector.select_frames(
        video_data=video,
        num_frames=num_keyframes
    )
    
    return keyframes
```

### 6.5 商业云服务

**Google Cloud Video Intelligence API**：提供自动化的视频分析服务，包括场景检测、内容标注等功能。

```python
from google.cloud import videointelligence_v1

def google_video_intelligence_keyframes(video_path):
    """使用Google Cloud Video Intelligence提取关键帧"""
    client = videointelligence_v1.VideoIntelligenceServiceClient()
    
    with open(video_path, 'rb') as f:
        input_content = f.read()
    
    features = [
        videointelligence_v1.Feature.SHOT_CHANGE_DETECTION,
        videointelligence_v1.Feature.EXPLICIT_CONTENT_DETECTION
    ]
    
    request = videointelligence_v1.AnnotateVideoRequest(
        input_content=input_content,
        features=features
    )
    
    operation = client.annotate_video(request=request)
    
    # 等待结果
    result = operation.result(timeout=600)
    
    # 提取镜头变化信息
    shot_changes = result.annotation_result[0].shot_label_annotations
    return shot_changes
```

**AWS Rekognition Video**：支持视频内容分析，包括人脸识别、物体检测等。

**Azure Video Indexer**：提供视频内容理解和元数据提取服务。

## 7. 评估指标

### 7.1 覆盖率指标

**帧覆盖率（Frame Coverage）**：关键帧对原视频内容的覆盖程度，通常通过计算关键帧与所有帧在特征空间中的覆盖率来衡量。

$$Coverage = \frac{|\bigcup_{i=1}^{k} N(f_i)|}{N}$$

其中$N(f_i)$是关键帧$f_i$的邻域（特征空间中相似的帧集合），$N$是视频总帧数。

**信息熵覆盖率**：基于信息熵评估关键帧集合的信息完整性。

### 7.2 精确率与召回率

精确率和召回率用于评估关键帧选择与人工标注的一致性。

**精确率（Precision）**：选中的关键帧中真正相关的比例。

$$Precision = \frac{TP}{TP + FP}$$

**召回率（Recall）**：所有相关帧中被正确选中的比例。

$$Recall = \frac{TP}{TP + FN}$$

**F1分数**：精确率和召回率的调和平均。

$$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$

### 7.3 压缩率与保真度

**压缩率（Compression Ratio）**：关键帧数量与原视频帧数的比值。

$$CR = \frac{N_{keyframes}}{N_{total}}$$

**保真度（Fidelity Score）**：关键帧集合与原视频在特征空间中的相似度。

$$Fidelity = \frac{1}{k} \sum_{i=1}^{k} \max_{j} sim(f_i, g_j)$$

其中$g_j$是原视频第$j$帧，$f_i$是关键帧。

### 7.4 多样性与代表性

**帧间多样性**：关键帧集合内部的多样性程度。

$$Diversity = \frac{2}{k(k-1)} \sum_{i<j} (1 - sim(f_i, f_j))$$

**代表性得分**：每帧对其所属聚类的代表性程度。

$$Representativeness = \frac{1}{|C|} \sum_{f \in C} sim(f, c)$$

其中$c$是聚类质心。

## 8. 方法对比与推荐

### 8.1 方法对比矩阵

| 方法 | 计算成本 | 准确性 | 语义理解 | 实时性 | 适用场景 |
|------|---------|--------|---------|--------|----------|
| **基于相似度** | 低 | 中 | 无 | 高 | 幻灯片转换、实时处理 |
| **直方图比较** | 低 | 中 | 无 | 高 | 颜色变化检测、镜头切换 |
| **边缘检测(ECR)** | 中 | 中 | 无 | 中 | 边缘丰富的场景 |
| **光流分析** | 中-高 | 中 | 运动模式 | 中 | 动态场景、运动分析 |
| **CNN特征** | 高 | 高 | 视觉语义 | 低 | 通用视频分析 |
| **Transformer** | 很高 | 很高 | 强语义 | 低 | 高精度需求场景 |
| **聚类方法** | 中-高 | 高 | 统计语义 | 中 | 内容聚类、摘要生成 |
| **MMR选择** | 高 | 高 | 多样性 | 低 | 摘要生成、检索 |
| **VLM方法** | 极高 | 极高 | 理解能力强 | 很低 | 语义理解需求 |

### 8.2 场景推荐

**场景1：讲座/会议视频转PDF**
- **推荐方案**：基于相似度的方法（extract-video-ppt）
- **参数设置**：similarity=0.5-0.6，每秒采样1帧
- **优势**：计算快速、输出稳定、适合批量处理

**场景2：视频内容检索**
- **推荐方案**：CNN特征 + MMR选择
- **参数设置**：ResNet特征，MMR λ=0.7
- **优势**：语义丰富、结果多样、覆盖全面

**场景3：监控视频摘要**
- **推荐方案**：场景检测 + 关键帧提取
- **工具推荐**：PySceneDetect + 直方图比较
- **优势**：实时处理、存储节省

**场景4：电影/纪录片摘要**
- **推荐方案**：Transformer + 注意力机制
- **优势**：理解叙事结构、捕捉重要情节

**场景5：直播流处理**
- **推荐方案**：轻量级CNN + 滑动窗口
- **优势**：低延迟、持续更新

### 8.3 实现建议

**初学者推荐**：从基于相似度的方法开始，实现简单、效果稳定、易于调试。

**生产环境**：建议采用分层架构——先用轻量级方法进行初筛，再用深度学习方法精筛。

**混合策略**：结合多种方法的优势，例如先用场景检测确定粗粒度边界，再在每个场景内使用聚类选择细粒度关键帧。

**参数调优**：建议建立验证集，通过F1分数或覆盖率指标自动搜索最优参数组合。

## 参考文献

1. Truong, B. T., & Venkatesh, S. (2007). Video abstraction: A systematic review and classification. ACM Transactions on Multimedia Computing, Communications, and Applications.

2. Wolf, W. (1996). Key frame selection by an algorithm. IEEE International Conference on Multimedia Computing and Systems.

3. Girgensohn, A., & Boreczky, J. (2000). Time-constrained keyframe selection. IEEE International Conference on Multimedia Computing and Systems.

4. Dosovitskiy, A., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. ICLR.

5. Radford, A., et al. (2021). Learning transferable visual models from natural language supervision. ICML.

## 附录：完整代码示例

```python
"""
关键帧提取工具类
整合多种方法，提供统一的接口
"""

import cv2
import numpy as np
from enum import Enum
from abc import ABC, abstractmethod

class ExtractionMethod(Enum):
    SIMILARITY = "similarity"
    HISTOGRAM = "histogram"
    EDGE = "edge"
    CNN = "cnn"
    CLUSTERING = "clustering"
    MMR = "mmr"

class KeyframeExtractor(ABC):
    """关键帧提取器基类"""
    
    @abstractmethod
    def extract(self, video_path, **kwargs):
        pass
    
    @abstractmethod
    def get_name(self):
        pass

class SimilarityExtractor(KeyframeExtractor):
    """基于相似度的关键帧提取器"""
    
    def __init__(self, threshold=0.6, fps=1):
        self.threshold = threshold
        self.fps = fps
    
    def extract(self, video_path, **kwargs):
        return self._extract_similarity(video_path)
    
    def _extract_similarity(self, video_path):
        cap = cv2.VideoCapture(video_path)
        keyframes = []
        last_frame = None
        
        fps_video = cap.get(cv2.CAP_PROP_FPS)
        interval = max(1, int(fps_video / self.fps))
        
        frame_count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % interval != 0:
                frame_count += 1
                continue
            
            if last_frame is not None:
                similarity = self._calculate_similarity(last_frame, frame)
                if similarity < self.threshold:
                    keyframes.append(frame.copy())
                    last_frame = frame.copy()
            else:
                keyframes.append(frame.copy())
                last_frame = frame.copy()
            
            frame_count += 1
        
        cap.release()
        return keyframes
    
    def _calculate_similarity(self, frame1, frame2):
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
        return cv2.matchTemplate(gray1, gray2, cv2.TM_CCOEFF_NORMED)[0][0]
    
    def get_name(self):
        return "Similarity-based Extractor"

class HistogramExtractor(KeyframeExtractor):
    """基于直方图的关键帧提取器"""
    
    def __init__(self, threshold=0.7):
        self.threshold = threshold
    
    def extract(self, video_path, **kwargs):
        cap = cv2.VideoCapture(video_path)
        keyframes = []
        last_hist = None
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            hist = cv2.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])
            cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)
            
            if last_hist is not None:
                similarity = 1 - cv2.compareHist(last_hist, hist, 
                                                   cv2.HISTCMP_BHATTACHARYYA)
                if similarity < self.threshold:
                    keyframes.append(frame.copy())
                    last_hist = hist
            else:
                keyframes.append(frame.copy())
                last_hist = hist
        
        cap.release()
        return keyframes
    
    def get_name(self):
        return "Histogram-based Extractor"

class KeyframeExtractionPipeline:
    """关键帧提取流水线"""
    
    def __init__(self, method=ExtractionMethod.SIMILARITY, **params):
        self.method = method
        self.params = params
        self.extractor = self._create_extractor()
    
    def _create_extractor(self):
        if self.method == ExtractionMethod.SIMILARITY:
            return SimilarityExtractor(**self.params)
        elif self.method == ExtractionMethod.HISTOGRAM:
            return HistogramExtractor(**self.params)
        else:
            raise ValueError(f"Unsupported method: {self.method}")
    
    def extract(self, video_path):
        return self.extractor.extract(video_path)
    
    def set_method(self, method, **params):
        self.method = method
        self.params = params
        self.extractor = self._create_extractor()

# 使用示例
if __name__ == "__main__":
    pipeline = KeyframeExtractionPipeline(
        method=ExtractionMethod.SIMILARITY,
        threshold=0.6,
        fps=1
    )
    
    keyframes = pipeline.extract("input_video.mp4")
    print(f"提取到 {len(keyframes)} 个关键帧")
```
